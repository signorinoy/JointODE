---
title: "Technical Details"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Technical Details}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This vignette describes the mathematical framework and computational implementation of the JointODE model, which jointly models longitudinal biomarker trajectories and survival outcomes through a coupled ordinary differential equation (ODE) system.

## Model Framework

### Longitudinal Model

The observed biomarker measurements are modeled as: $$
V_{ij}=m_i(T_{ij})+b_i+\varepsilon_{ij},\quad i=1,\ldots,n,\quad j=1,\ldots,n_i
$$

where:

-   $V_{ij}$: Observed biomarker value for subject $i$ at time $T_{ij}$
-   $m_i(t)$: True underlying biomarker trajectory
-   $b_i\sim\mathcal{N}(0,\sigma_{b}^{2})$: Subject-specific random intercept
-   $\varepsilon_{ij}\sim\mathcal{N}(0,\sigma_{e}^{2})$: Measurement error

The biomarker trajectory evolution is characterized by the following second-order differential equation:

$$
\ddot{m}_i(t) = f\big(m_i(t), \dot{m}_i(t), \mathbf{X}_i(t), t\big)
$$

where $f: \mathbb{R} \times \mathbb{R} \times \mathbb{R}^p \times \mathbb{R}^+ \to \mathbb{R}$ is a smooth function modeling the biomarker acceleration as a function of its current value $m_i(t)$, velocity $\dot{m}_i(t)$, time-varying covariates $\mathbf{X}_i(t) \in \mathbb{R}^p$, and time $t$.

### Survival Model

The hazard function incorporates biomarker dynamics:

$$
\lambda_i(t) = \lambda_{0}(t)\exp\left[\mathbf{m}_i(t)^{\top}\boldsymbol{\alpha}+\mathbf{W}_i^{\top}\boldsymbol{\phi}+b_{i}\right]
$$

where:

-   $\lambda_{0}(t)$: Baseline hazard (e.g., Weibull, piecewise constant)
-   $\mathbf{m}_i(t)=\left(m_i(t), \dot{m}_i(t), \ddot{m}_i(t)\right)^{\top}$: Biomarker value and derivatives
-   $\boldsymbol{\alpha}=(\alpha_0, \alpha_1, \alpha_2)^{\top}$: Association parameters for value, velocity, and acceleration
-   $\mathbf{W}_i$: Baseline covariates with coefficients $\boldsymbol{\phi}$
-   $b_i$: Subject-specific random intercept

### ODE System

The model tracks three state variables for each subject $i$:

- $\Lambda_i(t)$: Cumulative hazard
- $m_i(t)$: Biomarker level
- $\dot{m}_i(t)$: Biomarker velocity (rate of change)

The state vector $\mathbf{s}_i(t) = (\Lambda_i(t), m_i(t), \dot{m}_i(t))^{\top}$ evolves according to:

$$
\frac{d\mathbf{s}_i}{dt} = \begin{pmatrix}
\lambda_i(t|b_i) \\
\dot{m}_i(t) \\
g(\boldsymbol{\beta}^{\top}\mathbf{Z}_i(t))
\end{pmatrix}
$$

with initial conditions $\mathbf{s}_i(0) = (0, m_{i0}, \dot{m}_{i0})^{\top}$.

### Model Components

**Hazard Function:**
$$\lambda_i(t|b_i) = \exp\left[\boldsymbol{\eta}^{\top} \mathbf{B}^{(\lambda)}(t) + \boldsymbol{\alpha}^{\top}\mathbf{m}_i(t) + b_{i} + \mathbf{W}_i^{\top}\boldsymbol{\phi}\right]$$

- $\mathbf{B}^{(\lambda)}(t)$: B-spline basis for baseline hazard
- $\mathbf{m}_i(t) = (m_i(t), \dot{m}_i(t), \ddot{m}_i(t))^{\top}$: Biomarker features
- $b_i$: Subject-specific random effect
- $\mathbf{W}_i$: Baseline covariates

**Acceleration Function:**
$$g(u) = \boldsymbol{\theta}^{\top} \mathbf{B}^{(g)}(u), \quad u = \boldsymbol{\beta}^{\top}\mathbf{Z}_i(t)$$

- $\mathbf{B}^{(g)}(u)$: B-spline basis for acceleration
- $\mathbf{Z}_i(t) = (m_i(t), \dot{m}_i(t), \mathbf{X}_i(t)^{\top}, t)^{\top}$: Feature vector
- $\boldsymbol{\beta}$: Single-index coefficients (constrained: $\|\boldsymbol{\beta}\| = 1$)

## Statistical Inference

### Likelihood

The joint likelihood for subject $i$ integrates over the random effect:

$$L_i(\boldsymbol{\Theta}) = \int f(\mathbf{V}_i | b_i) \cdot f(T_i, \delta_i | b_i) \cdot f(b_i) \, db_i$$

where $\boldsymbol{\Theta} = (\boldsymbol{\theta}, \boldsymbol{\beta}, \boldsymbol{\eta}, \boldsymbol{\alpha}, \boldsymbol{\phi}, \sigma_e^2, \sigma_b^2)$.

**Likelihood Components:**

1. **Longitudinal:** $f(\mathbf{V}_i | b_i) = \prod_{j=1}^{n_i} \mathcal{N}(V_{ij}; m_i(T_{ij}) + b_i, \sigma_e^2)$
2. **Survival:** $f(T_i, \delta_i | b_i) = [\lambda_i(T_i|b_i)]^{\delta_i} \exp[-\Lambda_i(T_i|b_i)]$
3. **Random Effect:** $f(b_i) \sim \mathcal{N}(0, \sigma_b^2)$

### EM Algorithm

We use an Expectation-Maximization (EM) algorithm for parameter estimation.

#### E-Step: Posterior Computation

For each subject $i$, compute the posterior distribution of $b_i$ given observed data $\mathcal{O}_i$.

**Key simplification:** The hazard and cumulative hazard factor as:

- $\lambda_i(t|b_i) = e^{b_i} \lambda_i(t|0)$
- $\Lambda_i(t|b_i) = e^{b_i} \Lambda_i(t|0)$

**Implementation:**

1. **Solve baseline ODE** with $b_i = 0$ to obtain $m_i(t)$, $\lambda_i(t|0)$, $\Lambda_i(T_i|0)$
2. **Find posterior mode** $\tilde{b}_i$ by maximizing:
   $$\ell_i(b) = b\left[\frac{S_i}{\sigma_e^2} + \delta_i\right] - \frac{b^2}{2}\left[\frac{n_i}{\sigma_e^2} + \frac{1}{\sigma_b^2}\right] - e^b\Lambda_i(T_i|0)$$
   where $S_i = \sum_j(V_{ij} - m_i(T_{ij}))$
3. **Compute posterior moments** via adaptive Gauss-Hermite quadrature:
   - Mean: $\hat{b}_i = E[b_i|\mathcal{O}_i]$
   - Variance: $\hat{v}_i = \text{Var}[b_i|\mathcal{O}_i]$
   - Transform: $E[e^{b_i}|\mathcal{O}_i]$ for survival updates

#### M-Step: Parameter Updates

Maximize the expected complete-data log-likelihood:

$$Q(\boldsymbol{\Theta}) = Q_{\text{long}} + Q_{\text{surv}} + Q_{\text{RE}}$$

where:

- $Q_{\text{long}} = -\frac{1}{2\sigma_e^2}\sum_{i,j} [(V_{ij} - m_i(T_{ij}) - \hat{b}_i)^2 + \hat{v}_i] - \frac{N}{2}\log(2\pi\sigma_e^2)$
- $Q_{\text{surv}} = \sum_i [\delta_i(\log\lambda_i(T_i|0) + \hat{b}_i) - E[e^{b_i}|\mathcal{O}_i]\Lambda_i(T_i|0)]$
- $Q_{\text{RE}} = -\frac{1}{2\sigma_b^2}\sum_i (\hat{b}_i^2 + \hat{v}_i) - \frac{n}{2}\log(2\pi\sigma_b^2)$

**Optimization Strategy:**

1. **Survival parameters** $(\boldsymbol{\eta}, \boldsymbol{\alpha}, \boldsymbol{\phi})$:
   - Fix trajectory parameters
   - Optimize via L-BFGS with analytical gradients
2. **Trajectory parameters** $(\boldsymbol{\beta}, \boldsymbol{\theta})$:
   - Spherical parameterization enforces constraint $\|\boldsymbol{\beta}\| = 1$
   - Transform $\boldsymbol{\beta} \in \mathbb{S}^{n-1}$ to spherical coordinates $\boldsymbol{\varphi} \in \mathbb{R}^{n-1}$
   - Iterate until convergence:
     - Fix $\boldsymbol{\theta}$, optimize $\boldsymbol{\varphi}$ (unconstrained optimization)
     - Convert $\boldsymbol{\varphi} \to \boldsymbol{\beta}$ via spherical transformation
     - Fix $\boldsymbol{\beta}$, optimize $\boldsymbol{\theta}$
3. **Variance components** (closed-form):
   - $\sigma_e^2 = \frac{1}{N}\sum_{i,j}[(V_{ij} - m_i(T_{ij}) - \hat{b}_i)^2 + \hat{v}_i]$
   - $\sigma_b^2 = \frac{1}{n}\sum_i(\hat{b}_i^2 + \hat{v}_i)$

## Computational Details

### Gradient Computation

The M-step maximizes $Q(\boldsymbol{\Theta})$ using gradient-based optimization. All gradients decompose additively:
$$\nabla Q = \nabla Q_{\text{long}} + \nabla Q_{\text{surv}}$$

**Hazard parameters** ($\boldsymbol{\eta}$, $\boldsymbol{\alpha}$, $\boldsymbol{\phi}$) admit closed-form gradient expressions:

- $\nabla_{\boldsymbol{\eta}} Q = \sum_i [\delta_i \mathbf{B}^{(\lambda)}(T_i) - E[e^{b_i}|\mathcal{O}_i] \frac{\partial\Lambda_i(T_i|0)}{\partial\boldsymbol{\eta}}]$
- $\nabla_{\boldsymbol{\alpha}} Q = \sum_i [\delta_i \mathbf{m}_i(T_i) - E[e^{b_i}|\mathcal{O}_i] \frac{\partial\Lambda_i(T_i|0)}{\partial\boldsymbol{\alpha}}]$
- $\nabla_{\boldsymbol{\phi}} Q = \sum_i [\delta_i - E[e^{b_i}|\mathcal{O}_i]\Lambda_i(T_i|0)] \mathbf{W}_i$

Since $\Lambda_i(T_i|0)$ depends on $\boldsymbol{\eta}$ and $\boldsymbol{\alpha}$ through the hazard, their gradients are:

- $\frac{\partial\Lambda_i(T_i|0)}{\partial\boldsymbol{\eta}}=\int_{0}^{T_i}\frac{\partial\exp\left\{\boldsymbol{\eta}^{\top} \mathbf{B}^{(\lambda)}(t) + \boldsymbol{\alpha}^{\top}\mathbf{m}_i(t) + b_{i} + \mathbf{W}_i^{\top}\boldsymbol{\phi}\right\}}{\partial\boldsymbol{\eta}}\,\mathrm{d}t$
- $\frac{\partial\Lambda_i(T_i|0)}{\partial\boldsymbol{\alpha}}=\int_{0}^{T_i}\frac{\partial\exp\left\{\boldsymbol{\eta}^{\top} \mathbf{B}^{(\lambda)}(t) + \boldsymbol{\alpha}^{\top}\mathbf{m}_i(t) + b_{i} + \mathbf{W}_i^{\top}\boldsymbol{\phi}\right\}}{\partial\boldsymbol{\alpha}}\,\mathrm{d}t$
- $\frac{\partial\Lambda_i(T_i|0)}{\partial\boldsymbol{\phi}}=\int_{0}^{T_i}\frac{\partial\exp\left\{\boldsymbol{\eta}^{\top} \mathbf{B}^{(\lambda)}(t) + \boldsymbol{\alpha}^{\top}\mathbf{m}_i(t) + b_{i} + \mathbf{W}_i^{\top}\boldsymbol{\phi}\right\}}{\partial\boldsymbol{\phi}}\,\mathrm{d}t$

The computation of these gradients requires solving an augmented ODE system:

$$\frac{d}{dt}\begin{bmatrix}
\Lambda_i \\ m_i \\ \dot{m}_i \\ \partial\Lambda_{\eta,i} \\ \partial\Lambda_{\alpha,i}
\end{bmatrix} = \begin{bmatrix}
\lambda_i(t|0) \\ \dot{m}_i(t) \\ g(\boldsymbol{\beta}^{\top}\mathbf{Z}(t)) \\ \mathbf{B}^{(\lambda)}(t) \lambda_i(t|0) \\ \mathbf{m}(t) \lambda_i(t|0)
\end{bmatrix}$$

where $\partial\Lambda_{\eta,i}$ and $\partial\Lambda_{\alpha,i}$ are sensitivities with respect to $\boldsymbol{\eta}$ and $\boldsymbol{\alpha}$, respectively.

**Trajectory parameters** ($\boldsymbol{\varphi}$, $\boldsymbol{\theta}$) have gradient expressions involving sensitivities:

- For $\boldsymbol{\beta}$ parameters (optimized via spherical coordinates $\boldsymbol{\varphi}$):
  - $\nabla_{\boldsymbol{\beta}} Q = \sum_{i,j} \frac{r_{ij}}{\sigma_e^2} \frac{\partial m_i(T_{ij})}{\partial \boldsymbol{\beta}} + \sum_i \left[\delta_i \boldsymbol{\alpha}^{\top} \frac{\partial \mathbf{m}_i(T_i)}{\partial \boldsymbol{\beta}} - E[e^{b_i}|\mathcal{O}_i] \frac{\partial \Lambda_i(T_i|0)}{\partial \boldsymbol{\beta}}\right]$
  - $\nabla_{\boldsymbol{\varphi}} Q = \mathbf{J}^{\top} \nabla_{\boldsymbol{\beta}} Q$ where $\mathbf{J} = \frac{\partial \boldsymbol{\beta}}{\partial \boldsymbol{\varphi}}$ is the spherical Jacobian
- $\nabla_{\boldsymbol{\theta}} Q = \sum_{i,j} \frac{r_{ij}}{\sigma_e^2} \frac{\partial m_i(T_{ij})}{\partial \boldsymbol{\theta}} + \sum_i \left[\delta_i \boldsymbol{\alpha}^{\top} \frac{\partial \mathbf{m}_i(T_i)}{\partial \boldsymbol{\theta}} - E[e^{b_i}|\mathcal{O}_i] \frac{\partial \Lambda_i(T_i|0)}{\partial \boldsymbol{\theta}}\right]$

where $r_{ij} = V_{ij} - m_i(T_{ij}) - \hat{b}_i$ denotes the residual.

Since $m_i(t)$ depends on $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$, their gradients are:

$$\frac{\partial m_i(t)}{\partial \boldsymbol{\beta}} = \int_{0}^{t} \frac{\partial\dot{m}_i(s)}{\partial \boldsymbol{\beta}} \,\mathrm{d}s,\quad \frac{\partial\dot{m}_i(t)}{\partial \boldsymbol{\beta}} = \int_{0}^{t} \frac{\partial \ddot{m}(s)}{\partial \boldsymbol{\beta}} \,\mathrm{d}s,\quad \frac{\partial \ddot{m}(s)}{\partial \boldsymbol{\beta}} \,\mathrm{d}s = \boldsymbol{\theta}^{\top}\mathbf{B}'_g(u) \frac{\partial u}{\partial\boldsymbol{\beta}}$$

where

$$\frac{\partial u}{\partial\boldsymbol{\beta}} = \mathbf{Z}(t) + \boldsymbol{\beta}^{\top} \frac{\partial \mathbf{Z}(t)}{\partial\boldsymbol{\beta}} = \mathbf{Z}(t) + \boldsymbol{\beta}^{\top} \begin{bmatrix}
\frac{\partial m_i(t)}{\partial \boldsymbol{\beta}} & \frac{\partial \dot{m}_i(t)}{\partial \boldsymbol{\beta}} & \mathbf{0} & \mathbf{0}
\end{bmatrix}$$

Similarly,

$$\frac{\partial m_i(t)}{\partial \boldsymbol{\theta}} = \int_{0}^{t} \frac{\partial\dot{m}_i(s)}{\partial \boldsymbol{\theta}} \,\mathrm{d}s,\quad \frac{\partial\dot{m}_i(t)}{\partial \boldsymbol{\theta}} = \int_{0}^{t} \frac{\partial \ddot{m}(s)}{\partial \boldsymbol{\theta}} \,\mathrm{d}s,\quad \frac{\partial \ddot{m}(s)}{\partial \boldsymbol{\theta}} \,\mathrm{d}s = \mathbf{B}_g(u) + \boldsymbol{\theta}^{\top}\mathbf{B}'_g(u) \frac{\partial u}{\partial\boldsymbol{\theta}}$$

where

$$
\frac{\partial u}{\partial\boldsymbol{\theta}} = \boldsymbol{\beta}^{\top} \frac{\partial \mathbf{Z}(t)}{\partial\boldsymbol{\theta}} = \boldsymbol{\beta}^{\top} \begin{bmatrix}
\frac{\partial m_i(t)}{\partial \boldsymbol{\theta}} & \frac{\partial \dot{m}_i(t)}{\partial \boldsymbol{\theta}} & \mathbf{0} & \mathbf{0}
\end{bmatrix}
$$

The gradients with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$ similarly require solving augmented ODE systems that track sensitivities.

For $\boldsymbol{\beta}$ sensitivities:
$$\frac{d}{dt}\begin{bmatrix}
\Lambda_i \\ m_i \\ \dot{m}_i \\ \partial m_{i,\beta} \\ \partial\dot{m}_{i,\beta} \\ \partial\Lambda_{\beta,i}
\end{bmatrix} = \begin{bmatrix}
\lambda_i(t|0) \\ \dot{m}_i(t) \\ g(\boldsymbol{\beta}^{\top}\mathbf{Z}(t)) \\ \partial \dot{m}_{i,\beta} \\ \boldsymbol{\theta}^{\top}\mathbf{B}'_g(u) \left(\mathbf{Z}(t) + \boldsymbol{\beta}^{\top}\frac{\partial\mathbf{Z}(t)}{\partial\boldsymbol{\beta}}\right) \\ \boldsymbol{\alpha}^{\top}\frac{\partial\mathbf{m}_i(t)}{\partial\boldsymbol{\beta}} \lambda_i(t|0)
\end{bmatrix}$$

For $\boldsymbol{\theta}$ sensitivities:
$$\frac{d}{dt}\begin{bmatrix}
\Lambda_i \\ m_i \\ \dot{m}_i \\ \partial m_{i,\theta} \\ \partial\dot{m}_{i,\theta} \\ \partial\Lambda_{\theta,i}
\end{bmatrix} = \begin{bmatrix}
\lambda_i(t|0) \\ \dot{m}_i(t) \\ g(\boldsymbol{\beta}^{\top}\mathbf{Z}(t)) \\ \partial\dot{m}_{i,\theta} \\ \mathbf{B}_g(u) + \boldsymbol{\theta}^{\top}\mathbf{B}'_g(u) \boldsymbol{\beta}^{\top}\frac{\partial\mathbf{Z}(t)}{\partial\boldsymbol{\theta}} \\ \boldsymbol{\alpha}^{\top}\frac{\partial\mathbf{m}_i(t)}{\partial\boldsymbol{\theta}} \lambda_i(t|0)
\end{bmatrix}$$

where $\mathbf{B}'_g(u) = \frac{d\mathbf{B}_g(u)}{du}$ denotes the B-spline derivative basis.

### Adjoint Sensitivity Analysis

While the forward sensitivity approach above requires solving augmented ODE systems with dimension proportional to the number of parameters, the adjoint method offers a more efficient alternative for gradient computation, especially when the number of parameters is large.

#### Adjoint Method Formulation

For a general objective function $J$ that depends on the ODE solution:
$$J = \int_0^T L(t, \mathbf{s}(t), \boldsymbol{\theta}) \, dt + G(\mathbf{s}(T), \boldsymbol{\theta})$$

where $\mathbf{s}(t)$ satisfies the ODE system $\dot{\mathbf{s}} = f(t, \mathbf{s}, \boldsymbol{\theta})$, the gradient with respect to parameters $\boldsymbol{\theta}$ can be computed by solving the adjoint equation backward in time:

$$\frac{d\boldsymbol{\lambda}}{dt} = -\left(\frac{\partial f}{\partial \mathbf{s}}\right)^T \boldsymbol{\lambda} - \left(\frac{\partial L}{\partial \mathbf{s}}\right)^T$$

with terminal condition:
$$\boldsymbol{\lambda}(T) = \left(\frac{\partial G}{\partial \mathbf{s}}\right)^T_{t=T}$$

The gradient is then:
$$\frac{dJ}{d\boldsymbol{\theta}} = \int_0^T \left[\boldsymbol{\lambda}^T \frac{\partial f}{\partial \boldsymbol{\theta}} + \frac{\partial L}{\partial \boldsymbol{\theta}}\right] dt + \frac{\partial G}{\partial \boldsymbol{\theta}}$$

#### Application to JointODE Model

For our model, the objective function components are:

**Longitudinal contribution:**
$$J_{\text{long}} = \sum_{j=1}^{n_i} \frac{1}{2\sigma_e^2} (V_{ij} - m_i(T_{ij}) - \hat{b}_i)^2$$

**Survival contribution:**
$$J_{\text{surv}} = -\delta_i \log \lambda_i(T_i|0) + E[e^{b_i}|\mathcal{O}_i] \Lambda_i(T_i|0)$$

The adjoint equations for computing gradients with respect to $\boldsymbol{\beta}$ become:

$$\frac{d\boldsymbol{\lambda}}{dt} = -\begin{bmatrix}
0 & 0 & 0 \\
0 & -\frac{\partial g}{\partial m} \lambda_3 - \alpha_0 \lambda_1 \lambda_i(t|0) & -\lambda_2 - \frac{\partial g}{\partial \dot{m}} \lambda_3 - \alpha_1 \lambda_1 \lambda_i(t|0) \\
0 & 0 & 0
\end{bmatrix} \boldsymbol{\lambda} - \boldsymbol{r}(t)$$

where $\boldsymbol{r}(t)$ represents the contribution from observation times:
$$\boldsymbol{r}(t) = \sum_{j: T_{ij} = t} \frac{1}{\sigma_e^2} (V_{ij} - m_i(t) - \hat{b}_i) \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} \delta(t - T_{ij})$$

with terminal condition at $t = T_i$:
$$\boldsymbol{\lambda}(T_i) = \begin{bmatrix}
E[e^{b_i}|\mathcal{O}_i] \\
-\delta_i \alpha_0 / \lambda_i(T_i|0) \\
-\delta_i \alpha_1 / \lambda_i(T_i|0)
\end{bmatrix}$$

#### Advantages of the Adjoint Method

1. **Memory efficiency**: Only requires storing the forward trajectory and adjoint variables, not sensitivities for all parameters
2. **Computational efficiency**: For $p$ parameters, forward sensitivity requires solving $3(1 + p)$ ODEs, while adjoint only requires $3 + 3 = 6$ ODEs
3. **Numerical stability**: Avoids potential numerical issues from simultaneously tracking many sensitivities
4. **Checkpointing**: Can use checkpointing strategies to trade computation for memory

#### Implementation Considerations

The adjoint method requires:
1. **Forward pass**: Solve the original ODE system and store the trajectory
2. **Backward pass**: Solve the adjoint equations backward in time
3. **Quadrature**: Integrate to compute the gradient

For stiff problems, implicit methods can be used for both forward and backward passes. The adjoint equations inherit the stiffness properties of the forward system, so similar numerical methods apply.

#### Comparison with Forward Sensitivity

| Aspect | Forward Sensitivity | Adjoint Method |
|--------|-------------------|----------------|
| ODEs to solve | $3(1 + p)$ | $3 + 3$ |
| Memory requirement | $O(pNT)$ | $O(NT)$ |
| Best for | Few parameters, many objectives | Many parameters, few objectives |
| Implementation | Simpler, single forward pass | More complex, forward + backward |
| Gradient accuracy | Direct computation | Requires accurate interpolation |

For the JointODE model with many spline coefficients and trajectory parameters, the adjoint method can provide significant computational savings, especially during the M-step optimization where gradients are computed repeatedly.

### Convergence

The EM algorithm iterates until the relative change in log-likelihood falls below a specified threshold:

$$\frac{|\mathcal{L}^{(k+1)} - \mathcal{L}^{(k)}|}{|\mathcal{L}^{(k)}|} < \epsilon$$
